{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0f18bbcb5886f741d46354959da58d8da526571432f6aee42961ba13d9f9ef567",
   "display_name": "Python 3.8.3 64-bit ('coffea-env': conda)",
   "language": "python"
  },
  "metadata": {
   "interpreter": {
    "hash": "f18bbcb5886f741d46354959da58d8da526571432f6aee42961ba13d9f9ef567"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Example of Coffea with the Work Queue executor.\n",
    "#\n",
    "# To execute, start this application, and then start workers that\n",
    "# will connect to it and execute tasks.\n",
    "#\n",
    "# Note that, as written, this only processes 4 data chunks and\n",
    "# should complete in a short time.  For a real run,\n",
    "# change maxchunks=None in the main program below.\n",
    "#\n",
    "# For simple testing, you can run one worker manually:\n",
    "#    work_queue_worker -N coffea-wq-${USER}\n",
    "#\n",
    "# Then to scale up, submit lots of workers to your favorite batch system:\n",
    "#    condor_submit_workers -N coffea-wq-${USER} 32\n",
    "#\n",
    "##################################################################\n",
    "\n",
    "###############################################################\n",
    "# Sample processor class given in the Coffea manual.\n",
    "###############################################################\n",
    "\n",
    "from work_queue import *\n",
    "import uproot\n",
    "from coffea.nanoevents import NanoEventsFactory, BaseSchema\n",
    "\n",
    "# https://github.com/scikit-hep/uproot4/issues/122\n",
    "uproot.open.defaults[\"xrootd_handler\"] = uproot.source.xrootd.MultithreadedXRootDSource\n",
    "\n",
    "import awkward as ak\n",
    "from coffea import hist, processor\n",
    "\n",
    "# register our candidate behaviors\n",
    "from coffea.nanoevents.methods import candidate\n",
    "ak.behavior.update(candidate.behavior)\n",
    "\n",
    "class MyProcessor(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            \"sumw\": processor.defaultdict_accumulator(float),\n",
    "            \"mass\": hist.Hist(\n",
    "                \"Events\",\n",
    "                hist.Cat(\"dataset\", \"Dataset\"),\n",
    "                hist.Bin(\"mass\", \"$m_{\\mu\\mu}$ [GeV]\", 60, 60, 120),\n",
    "            ),\n",
    "        })\n",
    "\n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "\n",
    "    def process(self, events):\n",
    "\n",
    "        # Note: This is required to ensure that behaviors are registered\n",
    "        # when running this code in a remote task.        \n",
    "        ak.behavior.update(candidate.behavior)\n",
    "\n",
    "        output = self.accumulator.identity()\n",
    "\n",
    "        dataset = events.metadata['dataset']\n",
    "        muons = ak.zip({\n",
    "            \"pt\": events.Muon_pt,\n",
    "            \"eta\": events.Muon_eta,\n",
    "            \"phi\": events.Muon_phi,\n",
    "            \"mass\": events.Muon_mass,\n",
    "            \"charge\": events.Muon_charge,\n",
    "        }, with_name=\"PtEtaPhiMCandidate\")\n",
    "\n",
    "        cut = (ak.num(muons) == 2) & (ak.sum(muons.charge) == 0)\n",
    "        # add first and second muon in every event together\n",
    "        dimuon = muons[cut][:, 0] + muons[cut][:, 1]\n",
    "\n",
    "        output[\"sumw\"][dataset] += len(events)\n",
    "        output[\"mass\"].fill(\n",
    "            dataset=dataset,\n",
    "            mass=dimuon.mass,\n",
    "        )\n",
    "\n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator\n",
    "\n",
    "\n",
    "###############################################################\n",
    "# Collect and display setup info.\n",
    "###############################################################\n",
    "\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Example Coffea Analysis with Work Queue Executor\")\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "import shutil\n",
    "import getpass\n",
    "import os.path\n",
    "\n",
    "wq_env_tarball=\"coffea-env.tar.gz\"\n",
    "wq_wrapper_path=shutil.which('python_package_run')\n",
    "wq_master_name=\"coffea-wq-{}\".format(getpass.getuser())\n",
    "\n",
    "print(\"Master Name: -N \"+wq_master_name)\n",
    "print(\"Environment: \"+wq_env_tarball)\n",
    "print(\"Wrapper Path: \"+wq_wrapper_path)\n",
    "\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "\n",
    "###############################################################\n",
    "# Sample data sources come from CERN opendata.\n",
    "###############################################################\n",
    "\n",
    "fileset = {\n",
    "    'DoubleMuon': [\n",
    "        'root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_DoubleMuParked.root',\n",
    "        'root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012C_DoubleMuParked.root',\n",
    "    ],\n",
    "}\n",
    "\n",
    "###############################################################\n",
    "# Configuration of the Work Queue Executor\n",
    "###############################################################\n",
    "\n",
    "work_queue_executor_args = {\n",
    "\n",
    "    # Options are common to all executors:\n",
    "    'compression': 1,\n",
    "    'schema' : BaseSchema,\n",
    "    'skipbadfiles': False,      # Note that maxchunks only works if this is false.\n",
    " \n",
    "    # Options specific to Work Queue:\n",
    "\n",
    "    # Additional files needed by the processor, such as local code libraries.\n",
    "    # 'extra-input-files' : [ 'myproc.py', 'config.dat' ],\n",
    "\n",
    "    # Resources to allocate per task.\n",
    "    'resources-mode' : 'auto',  # Adapt task resources to what's observed.\n",
    "    'resource-monitor': True,   # Measure actual resource consumption\n",
    "\n",
    "    # With resources set to auto, these are the max values for any task.\n",
    "    'cores': 2,                  # Cores needed per task.\n",
    "    'disk': 2000,                # Disk needed per task (MB)\n",
    "    'memory': 2000,              # Memory needed per task (MB)\n",
    "    'gpus' : 0,                  # GPUs needed per task.\n",
    "\n",
    "    # Options to control how workers find this master.\n",
    "    'master-name': wq_master_name,\n",
    "    'port': 9123,     # Port for manager to listen on: if zero, will choose automatically.\n",
    "\n",
    "    # Options to control how the environment is constructed.\n",
    "    # The named tarball will be transferred to each worker\n",
    "    # and activated using the wrapper script.\n",
    "    'environment-file': wq_env_tarball,\n",
    "    'wrapper' : wq_wrapper_path,\n",
    "\n",
    "    # Debugging: Display output of task if not empty.\n",
    "    'print-stdout': True,\n",
    "\n",
    "    # Debugging: Display notes about each task submitted/complete.\n",
    "    'verbose': False,\n",
    "\n",
    "    # Debugging: Produce a lot at the master side of things.\n",
    "    'debug-log' : 'coffea-wq.log',\n",
    "}\n",
    "\n",
    "###############################################################\n",
    "# Run the analysis via run_uproot_job.\n",
    "###############################################################\n",
    "\n",
    "import time\n",
    "# We create the tasks queue using the default port. If this port is already\n",
    "# been used by another program, you can try setting port = 0 to use an\n",
    "# available port.\n",
    "\n",
    "\n",
    "\n",
    "# Create our workers\n",
    "workers = Factory(batch_type='local', manager_host_port=f'localhost:9123')  \n",
    "tstart = time.time()\n",
    "with workers:\n",
    "    output = processor.run_uproot_job(\n",
    "        fileset,\n",
    "        treename='Events',\n",
    "        processor_instance=MyProcessor(),\n",
    "        executor=processor.work_queue_executor,\n",
    "        executor_args=work_queue_executor_args,\n",
    "        chunksize=100000,\n",
    "\n",
    "        # Change this to None for a large run:\n",
    "        maxchunks=4,\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "elapsed = time.time() - tstart\n",
    "\n",
    "print(output)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}